#import tensorflow as tf
import numpy as np
import scipy
import torch.nn.functional as F
from scipy.stats import rankdata
from operator import itemgetter

np.random.seed(1234)
import os
import time
import datetime
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from builddata import *
from model import KGEModel
import pickle

# Parameters
# ==================================================
parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')

parser.add_argument("--data", default="./data/", help="Data sources.")
parser.add_argument("--name", default="WN18RR", help="Name of the dataset.")
parser.add_argument("--model_name", default='expNoName', help="")
parser.add_argument("--embedding_dim", default=50, type=int, help="Dimensionality of character embedding")
parser.add_argument("--ConvL1FiltersNum",default=250,type=int,help="number of filter in convolution layer 1")
parser.add_argument("--ConvL2FiltersNum",default=10,type=int,help="number of filter in convolution layer 2")
parser.add_argument("--gamma", default=2.4, type=float, help="Gamma value")
parser.add_argument("--learning_rate", default=0.01, type=float, help="Learning rate")
parser.add_argument("--batch_size", default=64, type=int, help="Batch Size")
parser.add_argument("--neg_ratio", default=1, type=float, help="Number of negative triples generated by positive")
parser.add_argument("--num_epochs", default=3000, type=int, help="Number of training epochs")
parser.add_argument("--num_splits", default=8, type=int, help="Split the validation set into 8 parts for a faster evaluation")
parser.add_argument("--testIdx", default=1, type=int, help="From 0 to 7. Index of one of 8 parts")
parser.add_argument('--fold',default='1')
parser.add_argument("--dpo1", action='store_true',help="use dropout in CNN1")
parser.add_argument("--dpo2", action='store_true',help="use dropout in CNN2")
parser.add_argument("--bn1", action='store_true',help="use batch normalization in CNN1")
parser.add_argument("--bn2", action='store_true',help="use batch normalization in CNN1")
parser.add_argument("--alpha",default = 0.0)

#parser.add_argument("--run_folder", default="../", help="Data sources.")
#parser.add_argument("--filter_sizes", default="1", help="Comma-separated filter sizes")
#parser.add_argument("--num_filters", default=500, type=int, help="Number of filters per filter size")
#parser.add_argument("--dropout_keep_prob", default=1.0, type=float, help="Dropout keep probability")
#parser.add_argument("--l2_reg_lambda", default=0.001, type=float, help="L2 regularization lambda")
#parser.add_argument("--is_trainable", default=True, type=bool, help="")
#parser.add_argument("--saveStep", default=200, type=int, help="")
#parser.add_argument("--allow_soft_placement", default=True, type=bool, help="Allow device soft device placement")
#parser.add_argument("--log_device_placement", default=False, type=bool, help="Log placement of ops on devices")

parser.add_argument("--useConstantInit", action='store_true')

#parser.add_argument("--model_index", default='200', help="")
parser.add_argument("--decode", action='store_false')

args = parser.parse_args()
# print(args)


# Load data
#print("Loading data...")

train, valid, test, words_indexes, indexes_words, \
headTailSelector, entity2id, id2entity, relation2id, id2relation = build_data(path=args.data, name=args.name)

# fold = args.fold
# train, test, words_indexes, indexes_words, headTailSelector, entity2id, id2entity, relation2id, id2relation \
#     = custom_build_data(name=args.name,path='./data',fold=fold)

# exit(0)
data_size = len(train)
train_batch = Batch_Loader(train, words_indexes, indexes_words, headTailSelector, \
                           entity2id, id2entity, relation2id, id2relation, batch_size=args.batch_size,
                           neg_ratio=args.neg_ratio)

entity_array = np.array(list(train_batch.indexes_ents.keys()))
#print("Loading data... finished!")

x_train = np.array(list(train.keys())).astype(np.int32)
y_train = np.array(list(train.values())).astype(np.float32)
len_train = len(x_train)
batch_train = int(len_train / (args.num_splits - 1))

x_valid = np.array(list(valid.keys())).astype(np.int32)
y_valid = np.array(list(valid.values())).astype(np.float32)
len_valid = len(x_valid)
batch_valid = int(len_valid / (args.num_splits - 1))

x_test = np.array(list(test.keys())).astype(np.int32)
y_test = np.array(list(test.values())).astype(np.float32)
len_test = len(x_test)
batch_test = int(len_test / (args.num_splits - 1))
#batch_test = args.batch_size


# print(id2entity)
# entityid = list(id2entity.keys())
# exit(0)
# uncomment when tuning hyper-parameters on the validation set

# x_test = x_valid
# y_test = y_valid
# len_test = len_valid
# batch_test = batch_valid

# print(len_test)
# exit(0)
##########################################

# path of text file that store evaluation result
# file_path = os.path.join(os.getcwd(),'experiment_result',args.model_name,args.name+fold)

# for WN18RR dataset
# rel_dict = {
#             1:'_hypernym',
#             4:'_derivationally_related_form',
#             9:'_instance_hypernym',
#             26:'_also_see',
#             35:'_member_meronym',
#             78:'_synset_domain_topic_of',
#             89:'_has_part',
#             112:'_member_of_domain_usage',
#             141:'_member_of_domain_region',
#             549:'_verb_group',
#             862:'_similar_to'
#         }

# for Algo-Citation Dataset
# rel_dict = {
#     1: 'EXTEND',
#     4: 'USE',
#     13: 'MENTION'
# }

kge_model = KGEModel(
    #model_name=args.model,
    #hidden_dim=args.hidden_dim,
    #hidden_dim=50,  # just for debugging (remove this line later)
    vocab_size=len(words_indexes),
    embedding_size=args.embedding_dim,
    gamma=args.gamma,
    #gamma=2.4, # just for debugging (remove this line later)
    batch_size = args.batch_size,
    neg_ratio=args.neg_ratio,
    dpo1=args.dpo1,
    dpo2=args.dpo2,
    bn1 = args.bn1,
    bn2 = args.bn2,
    channel1_num=args.ConvL1FiltersNum,
    channel2_num=args.ConvL2FiltersNum
)

#  args.name, args.model_name+fold+'-'+str(epoch+1)+'epochs'
model_path = os.path.join(os.getcwd(),'models',args.name+'_'+args.model_name+fold+'-'+str(args.num_epochs)+'epochs'+'.pth')
# model_path = os.path.join(os.getcwd(),'models',args.name+'_'+args.model_name+fold+'.pth')
kge_model.load_state_dict(torch.load(model_path))
kge_model = kge_model.cuda()

# set all parameters not to require grad
for param in kge_model.parameters():
    param.requires_grad = False

kge_model.eval()
#print(kge_model)
print('load model finished')

def predict(x_batch, y_batch):
    h = torch.tensor(x_batch[:,0],dtype=torch.long).cuda().reshape(-1,1)
    r = torch.tensor(x_batch[:, 1],dtype=torch.long).cuda().reshape(-1,1)
    t = torch.tensor(x_batch[:, 2],dtype=torch.long).cuda().reshape(-1,1)

    return kge_model(h,r,t,True).cpu().detach().numpy()

def test_prediction(x_batch, y_batch, head_or_tail='head'):
    filterlist = []
    rel_list = []
    hits10 = 0.0
    mrr = 0.0
    mr = 0.0
    batch_len = int(len(x_batch))
    #batch_len = len(x_batch)
    print('batch len',batch_len)
    #exit(0)
    #print(x_batch)
    #print('total batch: ',batch_len)
    #print('total entity: ',len(entity2id))
    for i in range(batch_len):
    # for i in range(5):
        #start = time.time()
        new_x_batch = np.tile(x_batch[i], (len(entity_array), 0)) # change 0 to 1 to fix bug of len( new_x_batch[:, 0]) != len(entity_array)
        new_y_batch = np.tile(y_batch[i], (len(entity_array), 0))
        if head_or_tail == 'head':
            # print(len(entity2id))
            # print(len(new_x_batch[:,0]))
            # print(len(entity_array[:len(new_x_batch)]))
            new_x_batch[:, 0] = entity_array
        else:  # 'tail'
            new_x_batch[:, 2] = entity_array

        lstIdx = []
        for tmpIdxTriple in range(len(new_x_batch)):
            tmpTriple = (new_x_batch[tmpIdxTriple][0], new_x_batch[tmpIdxTriple][1],
                         new_x_batch[tmpIdxTriple][2])
            if (tmpTriple in train) or (tmpTriple in valid) or (
                    tmpTriple in test):  # also remove the valid test triple
                lstIdx.append(tmpIdxTriple)
        new_x_batch = np.delete(new_x_batch, lstIdx, axis=0)
        new_y_batch = np.delete(new_y_batch, lstIdx, axis=0)

        # thus, insert the valid test triple again, to the beginning of the array
        new_x_batch = np.insert(new_x_batch, 0, x_batch[i],
                                axis=0)  # thus, the index of the valid test triple is equal to 0
        new_y_batch = np.insert(new_y_batch, 0, y_batch[i], axis=0)

        # while len(new_x_batch) % ((int(args.neg_ratio) + 1) * args.batch_size) != 0:
        #    new_x_batch = np.append(new_x_batch, [x_batch[i]], axis=0)
        #    new_y_batch = np.append(new_y_batch, [y_batch[i]], axis=0)

        results = []
        rel_type = ''
        #listIndexes = range(0, len(new_x_batch), 50) # for 1vs50 evaluation
        listIndexes = range(0, len(new_x_batch), args.batch_size)
        #print('list index = ',len(listIndexes))
        #print(listIndexes)
        #exit(0)
        for tmpIndex in range(len(listIndexes) - 1):
        #for tmpIndex in range(1): # use 1vs50 evaluation
            #print('...')
            #print(len(new_x_batch[listIndexes[tmpIndex]:listIndexes[tmpIndex + 1]]))
            #exit(0)
            # print(new_x_batch[listIndexes[tmpIndex]:listIndexes[tmpIndex + 1]])
            rel_type, score = predict(
                new_x_batch[listIndexes[tmpIndex]:listIndexes[tmpIndex + 1]],
                new_y_batch[listIndexes[tmpIndex]:listIndexes[tmpIndex + 1]])
            results = np.append(results, score)

        #results = np.append(results,predict(new_x_batch[listIndexes[-1]:], new_y_batch[listIndexes[-1]:]))

        results = np.reshape(results, -1)
        results_with_id = rankdata(results, method='ordinal')
        _filter = results_with_id[0]
        #print('result shape: ', len(results))
        filterlist.append(_filter)
        rel_list.append(rel_type)
        print('filter is ', _filter)
        # print(results)
        #print(results_with_id)


        mr += _filter
        mrr += 1.0 / _filter
        if _filter <= 10:
            hits10 += 1
        print('at batch ',i,': ',np.array([mr, mrr, hits10]))

        #print()
        #if i > 10:
        #    break
        #finish = time.time()
        #interval = finish - start
        #print(i + 1, 'batch from ', len(x_batch))
        #print('finish in ',interval/60,' mins')
        #print('filter = ',_filter,'mrr = ',mrr)
    # exit(0)
    return np.array([mr, mrr, hits10]),filterlist,rel_list

#print(len(x_test[batch_test * args.testIdx: batch_test * (args.testIdx + 1)]))
#print(args.testIdx,args.num_splits-1)
#exit(0)

print('start evaluation')
# #start = time.time()
# totaltime = 0.0

#
if args.testIdx < (args.num_splits - 1):
    start = time.time()
    # head_results,filterlist, rel_list = test_prediction(
    #     x_test[batch_test * args.testIdx: batch_test * (args.testIdx + 1)],
    #     y_test[batch_test * args.testIdx: batch_test * (args.testIdx + 1)],
    #     head_or_tail='head')

    head_results, filterlist, rel_list = test_prediction(
        x_test,
        y_test,
        head_or_tail='head')
    # print(filterlist)
    # print(rel_list)
    # exit(0)
    # this is for validation check only...
    #head_results = test_prediction(
    #    x_valid[batch_test * args.testIdx: batch_test * (args.testIdx + 1)],
    #    y_valid[batch_test * args.testIdx: batch_test * (args.testIdx + 1)],
    #    head_or_tail='head')
    end = time.time()
    finishhrs = time.localtime(end)
    print('time spent to evaluate head result is ',(end-start)/60,' mins or ',(end-start)/3600,' hours, and finish at ', time.strftime('%H:%M:%S',finishhrs))
    # totaltime = totaltime + (end-start)
    start = time.time()
    # tail_results,filterlist2,rel_list2 = test_prediction(
    #     x_test[batch_test * args.testIdx: batch_test * (args.testIdx + 1)],
    #     y_test[batch_test * args.testIdx: batch_test * (args.testIdx + 1)],
    #     head_or_tail='tail')

    tail_results, filterlist2, rel_list2 = test_prediction(
        x_test,
        y_test,
        head_or_tail='tail')
    end = time.time()
    finishhrs = time.localtime(end)
    print('time spent to evaluate tail result is ', (end - start) / 60, ' mins or ', (end - start) / 3600,' hours, and finish at ', time.strftime('%H:%M:%S',finishhrs))
    # totaltime = totaltime + (end - start)
else:
    start = time.time()
    # head_results,filterlist, rel_list  = test_prediction(x_test[batch_test * args.testIdx: len_test],
    #                                y_test[batch_test * args.testIdx: len_test],
    #                                head_or_tail='head')
    head_results, filterlist, rel_list = test_prediction(x_test,
                                                         y_test,
                                                         head_or_tail='head')
    end = time.time()
    finishhrs = time.localtime(end)
    finishhrs = time.localtime(end)
    print('time spent to evaluate head result is ', (end - start) / 60, ' mins or ', (end - start) / 3600, ' hours, and finish at ',time.strftime('%H:%M:%S',finishhrs))
    # totaltime = totaltime + (end - start)

    start = time.time()
    # tail_results,filterlist2,rel_list2 = test_prediction(x_test[batch_test * args.testIdx: len_test],
    #                                y_test[batch_test * args.testIdx: len_test],
    #                                head_or_tail='tail')
    tail_results, filterlist2, rel_list2 = test_prediction(x_test,
                                                         y_test,
                                                         head_or_tail='tail')

    end = time.time()
    finishhrs = time.localtime(end)
    print('time spent to evaluate tail result is ', (end - start) / 60, ' mins or ', (end - start) / 3600, ' hours, and finish at ',time.strftime('%H:%M:%S',finishhrs))
    totaltime = totaltime + (end - start)

print('finish all in ',totaltime/60.0,' mins or ',totaltime/3600,' hours')
#exit(0) # remove later
print('args: \n', args)
print(kge_model)
'''
print('head result')
for _val in head_results:
    print(str(_val))
print('tail result')
for _val in tail_results:
    print(str(_val))
'''
#
# if not os.path.exists(file_path):
#     os.makedirs(file_path)
#
# wri = open(file_path + '/eval.test.' + fold + '.txt', 'w')
# for _val in head_results:
#     wri.write(str(_val) + ' ')
# wri.write('\n')
# for _val in tail_results:
#     wri.write(str(_val) + ' ')
# wri.write('\n')
# wri.close()
#
# wri2 = open(file_path+'/eval.test.allrank.'+ fold + '.txt', 'w')
# for i in range(len(filterlist)):
#     wri2.write(rel_list[i]+'\t'+str(filterlist[i])+'\n')
#     # wri2.write(str(filterlist[i]) + ' ')
# wri2.write('\n')
# for i in range(len(filterlist2)):
#     wri2.write(rel_list2[i]+'\t'+str(filterlist2[i])+'\n')
#     # wri2.write(str(filterlist2[i]) + ' ')
# wri2.write('\n')
# wri2.close()
#
# for _val in filterlist:
#     wri2.write(str(_val) + ' ')
# wri2.write('\n')
# for _val in filterlist2:
#     wri2.write(str(_val) + ' ')
# wri2.write('\n')
# wri2.write(str(filterlist))
# wri2.write('\n')
# wri2.write(str(filterlist2))
# wri2.write('\n')
