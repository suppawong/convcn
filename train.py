#import tensorflow as tf
import numpy as np
import scipy
import torch
import json

from model import KGEModel

np.random.seed(1234)
import os
import time
import datetime
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
#import matplotlib.pyplot as plt
from builddata import *
#from model import ConvKB

def save_model(model, optimizer, dataset,model_name):
    '''
    torch.save({
        **save_variable_list,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()},
        os.path.join(os.getcwd(),'models\\test\\checkpoint\\model.pth')
        #'./models/test/checkpoint', 'checkpoint'
    )
    '''
    torch.save(model.state_dict(),os.path.join(os.getcwd(),'models',dataset+'_'+model_name+'.pth'))

# Parameters
# ==================================================
parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')

parser.add_argument("--data", default="data/", help="Data sources.")
parser.add_argument("--name", default="CiteSeer_umd_5_fold", help="Name of the dataset.")
parser.add_argument("--model_name", default='expNoName', help="")
parser.add_argument("--embedding_dim", default=50, type=int, help="Dimensionality of character embedding")
parser.add_argument("--ConvL1FiltersNum",default=250,type=int,help="number of filter in convolution layer 1")
parser.add_argument("--ConvL2FiltersNum",default=10,type=int,help="number of filter in convolution layer 2")
parser.add_argument("--gamma", default=2.4, type=float, help="Gamma value")
parser.add_argument("--learning_rate", default=0.001, type=float, help="Learning rate")
parser.add_argument("--batch_size", default=64, type=int, help="Batch Size")
parser.add_argument("--neg_ratio", default=1, type=float, help="Number of negative triples generated by positive")
parser.add_argument("--num_epochs", default=5, type=int, help="Number of training epochs")
parser.add_argument("--num_splits", default=8, type=int, help="Split the validation set into 8 parts for a faster evaluation")
parser.add_argument("--testIdx", default=1, type=int, help="From 0 to 7. Index of one of 8 parts")
parser.add_argument('--fold',default='1')
parser.add_argument("--dpo1", action='store_true',help="use dropout in CNN1")
parser.add_argument("--dpo2", action='store_true',help="use dropout in CNN2")
parser.add_argument("--bn1", action='store_true',help="use batch normalization in CNN1")
parser.add_argument("--bn2", action='store_true',help="use batch normalization in CNN1")
#parser.add_argument("--run_folder", default="../", help="Data sources.")
#parser.add_argument("--filter_sizes", default="1", help="Comma-separated filter sizes")
#parser.add_argument("--num_filters", default=500, type=int, help="Number of filters per filter size")
#parser.add_argument("--dropout_keep_prob", default=1.0, type=float, help="Dropout keep probability")
#parser.add_argument("--l2_reg_lambda", default=0.001, type=float, help="L2 regularization lambda")
#parser.add_argument("--is_trainable", default=True, type=bool, help="")
#parser.add_argument("--saveStep", default=200, type=int, help="")
#parser.add_argument("--allow_soft_placement", default=True, type=bool, help="Allow device soft device placement")
#parser.add_argument("--log_device_placement", default=False, type=bool, help="Log placement of ops on devices")

#parser.add_argument("--useConstantInit", action='store_true')

#parser.add_argument("--model_index", default='200', help="")
#parser.add_argument("--decode", action='store_false')

args = parser.parse_args()
print(args)
# Load data
print("Loading data...")

# name = CiteSeer_umd_5_fold, CiteULike_5_fold, TopicCitation_5_fold, AlgoCitation_5_fold
# fold = 1,2,3,4,5
fold = args.fold

train, test, words_indexes, indexes_words, headTailSelector, entity2id, id2entity, relation2id, id2relation \
    = custom_build_data(name=args.name,path='./data',fold=fold)



# train, valid, test, words_indexes, indexes_words, \
# headTailSelector, entity2id, id2entity, relation2id, id2relation = build_data(path=args.data, name=args.name)
data_size = len(train)
# valid_size = len(valid)

# print(train.keys())

# txt = ''
# for triplet in train.keys():
#     txt = txt+ str(triplet[0]) + '\t' + str(triplet[1]) + '\t' + str(triplet[2]) +'\n'
#
# file = open('data/'+args.name+'/train_tmp_fold'+args.fold+'.txt','w')
# file.write(txt)
# # print(txt)
# exit(0)

print("Loading data... finished!")

# x_valid = np.array(list(valid.keys())).astype(np.int32)
# y_valid = np.array(list(valid.values())).astype(np.float32)

x_test = np.array(list(test.keys())).astype(np.int32)
y_test = np.array(list(test.values())).astype(np.float32)

# put the following code in the loop
train_batch = Batch_Loader(train, words_indexes, indexes_words, headTailSelector, \
                           entity2id, id2entity, relation2id, id2relation, batch_size=args.batch_size,
                           neg_ratio=args.neg_ratio)
# valid_batch = Batch_Loader(valid, words_indexes, indexes_words, headTailSelector, \
#                            entity2id, id2entity, relation2id, id2relation, batch_size=args.batch_size,
#                            neg_ratio=args.neg_ratio)

# print(train_batch())
# exit(0)

# Training

# print(len(words_indexes))
# exit(0)
kge_model = KGEModel(
    #model_name=args.model,
    #hidden_dim=args.hidden_dim,
    #hidden_dim=50,  # just for debugging (remove this line later)
    vocab_size=len(words_indexes),
    embedding_size=args.embedding_dim,
    gamma=args.gamma,
    #gamma=2.4, # just for debugging (remove this line later)
    batch_size = args.batch_size,
    neg_ratio=args.neg_ratio,
    dpo1=args.dpo1,
    dpo2=args.dpo2,
    bn1 = args.bn1,
    bn2 = args.bn2,
    channel1_num=args.ConvL1FiltersNum,
    channel2_num=args.ConvL2FiltersNum
)

# remove later...
#model_path = os.path.join(os.getcwd(),'models',args.name+'_'+args.model_name+'.pth')
#kge_model.load_state_dict(torch.load(model_path))

print(kge_model)
#exit(0)
'''
if args.do_train:
    # Set training configuration
    current_learning_rate = args.learning_rate
    optimizer = torch.optim.Adam(
        filter(lambda p: p.requires_grad, kge_model.parameters()),
        lr=current_learning_rate
    )
    if args.warm_up_steps:
        warm_up_steps = args.warm_up_steps
    else:
        warm_up_steps = args.max_steps // 2

if args.init_checkpoint:
    # Restore model from checkpoint directory
    logging.info('Loading checkpoint %s...' % args.init_checkpoint)
    checkpoint = torch.load(os.path.join(args.init_checkpoint, 'checkpoint'))
    init_step = checkpoint['step']
    kge_model.load_state_dict(checkpoint['model_state_dict'])
    if args.do_train:
        current_learning_rate = checkpoint['current_learning_rate']
        warm_up_steps = checkpoint['warm_up_steps']
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
else:
    logging.info('Ramdomly Initializing %s Model...' % args.model)
    init_step = 0
'''
kge_model.cuda()
# Set training configuration
current_learning_rate = args.learning_rate
optimizer = torch.optim.Adam(
    filter(lambda p: p.requires_grad, kge_model.parameters()),
    lr=current_learning_rate#,weight_decay=0.00005 #weight decay is l2 normailzation
)

# Restore model from checkpoint directory
'''
logging.info('Loading checkpoint %s...' % args.init_checkpoint)
checkpoint = torch.load(os.path.join(args.init_checkpoint, 'checkpoint'))
init_step = checkpoint['step']
kge_model.load_state_dict(checkpoint['model_state_dict'])
if args.do_train:
    current_learning_rate = checkpoint['current_learning_rate']
    warm_up_steps = checkpoint['warm_up_steps']
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
'''
init_step = 0 # just for debugging (remove this line later)
#step = init_step

logging.info('Start Training...')

#training_logs = [] # just for debugging (remove this line later)
#save_model(kge_model, optimizer, args.name, args.model_name)
#exit(0)
# Training Loop (may change the loop to use epochs if advisor tells to do

totaltime = 0.0

losslist = []
# lossvalidlist = []
iterlist = []
i = 1
num_batches_per_epoch = int((data_size - 1) / args.batch_size) + 1
# num_valid_batch = int((valid_size-1)/args.batch_size)+1

file_path = os.getcwd() #os.path.join(os.getcwd(),args.model_name,args.name)

for epoch in range(init_step,args.num_epochs):
    total_batch = len(range(num_batches_per_epoch))
    start = time.time()

    #kge_model = kge_model.train()
    #for param in kge_model.parameters():
    #    param.requires_grad = True

    kge_model.train()
    for param in kge_model.parameters():
        param.requires_grad = True

    for batch_num in range(num_batches_per_epoch):
        #pos_loss, neg_loss, loss, optimizerret = kge_model.train_step(kge_model, optimizer, train_batch)
        #
        #exit(0)
        loss, optimizerret = kge_model.train_step(kge_model, optimizer, train_batch)
        optimizer = optimizerret

    # print('train round 1 finish')
    # exit(0)
    # kge_model.eval()
    # for param in kge_model.parameters():
    #     param.requires_grad = False
    #
    # for batch_num in range(num_valid_batch):
    #     #for batch_num in range(num_valid_batch):
    #     with torch.no_grad():
    #         loss_valid, optimizerret = kge_model.train_step(kge_model, optimizer, valid_batch, valid_check=True)
            #pos_loss_valid, neg_loss_valid, loss_valid, optimizerret = kge_model.train_step(kge_model, optimizer, valid_batch,valid_check=True)
            #optimizer = optimizerret

        #i = i+1
        #training_logs.append(log)
        #print(loss)
        #print('at epoch ',epoch+1,': finish batch ',batch_num+1,' from total ',total_batch)


    #kge_model = kge_model.eval()
    #for param in kge_model.parameters():
    #    param.requires_grad = False

    #for batch_num in range(num_valid_batch):
    #    with torch.no_grad():
            #pos_loss_valid, neg_loss_valid, loss_valid = kge_model.train_step(kge_model, optimizer, valid_batch,valid_check=True)
    #        loss_valid = kge_model.train_step(kge_model, optimizer, valid_batch, valid_check=True)



    now = time.time()
    finishhrs = time.localtime(now)
    intervaltime = (now-start)/60.0
    totaltime = totaltime+intervaltime
    print('finish epoch ', epoch+1,' in ',intervaltime,' mins or ',intervaltime/60.0,' hour(s), and finish at ', time.strftime('%H:%M:%S',finishhrs))
    #print('pos loss: ',pos_loss,'\tneg loss: ',neg_loss,'\tloss:',loss)
    #print('loss is ',loss)
    #print('pos valid loss: ', pos_loss_valid, '\tneg valid loss: ', neg_loss_valid, '\tvalid loss:', loss_valid,'\n')

    losslist.append(loss)
    # lossvalidlist.append(loss_valid)
    iterlist.append(epoch + 1)

    # exit(0)
    write = open(file_path + '/' + 'lossvalLog_' + args.name + '_'+args.model_name+'.txt', 'a')
    write.write('at epoch '+ str(epoch + 1)+ '\n')
    write.write('train loss: ' + str(losslist) + '\n')
    # write.write('validation loss: ' + str(lossvalidlist) + '\n')
    write.write(str(iterlist) + '\n\n')
    #save_model(kge_model, optimizer, args.name, args.model_name)
    #input('press enter to go to next batch')
    #if (epoch+1)%3==0:
    #    save_model(kge_model, optimizer, args.name, args.model_name)
    if (epoch+1)%500 == 0: #and (epoch+1)%2 == 0:
        save_model(kge_model, optimizer, args.name, args.model_name+fold+'-'+str(epoch+1)+'epochs')
        #save_model(kge_model, optimizer, args.name, args.model_name)
        #input('press enter to go to next epoch')
        #exit(0) # 10 epoches is just for testing
    #exit(0)

save_model(kge_model, optimizer, args.name, args.model_name+fold)

#print('final loss is ', loss)
#plt.plot(iterlist, losslist)
#plt.plot(iterlist,lossvalidlist)
#plt.show()

print('total time spent to train the model is',totaltime, 'mins or ',totaltime/60.0, ' hour(s)')
print('model parameters')
print(args)

# print(losslist)
# print(lossvalidlist)
# print(iterlist)



'''
if args.do_valid:
    logging.info('Evaluating on Valid Dataset...')
    metrics = kge_model.test_step(kge_model, valid_triples, all_true_triples, args)
    log_metrics('Valid', step, metrics)

if args.do_test:
    logging.info('Evaluating on Test Dataset...')
    metrics = kge_model.test_step(kge_model, test_triples, all_true_triples, args)
    log_metrics('Test', step, metrics)

if args.evaluate_train:
    logging.info('Evaluating on Training Dataset...')
    metrics = kge_model.test_step(kge_model, train_triples, all_true_triples, args)
    log_metrics('Test', step, metrics)
'''