#import tensorflow as tf
import numpy as np
import scipy
import torch.nn.functional as F
from scipy.stats import rankdata
from operator import itemgetter

np.random.seed(1234)
import os
import time
import datetime
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from builddata import *
from model import KGEModel
import pickle
from Paper import *


class Paper:
    def __init__(self, id):
        self.id = id
        self.test_cited_paper = []
        self.train_cited_paper = []à¸³

    def add_test_cited_paper(self, cited_paper_id):
        self.test_cited_paper.append(cited_paper_id)

    def add_train_cited_paper(self, cited_paper_id):
        self.train_cited_paper.append(cited_paper_id)


# Parameters
# ==================================================
parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')

parser.add_argument("--data", default="./data/", help="Data sources.")
parser.add_argument("--name", default="WN18RR", help="Name of the dataset.")
parser.add_argument("--model_name", default='expNoName', help="")
parser.add_argument("--embedding_dim", default=50, type=int, help="Dimensionality of character embedding")
parser.add_argument("--ConvL1FiltersNum",default=250,type=int,help="number of filter in convolution layer 1")
parser.add_argument("--ConvL2FiltersNum",default=10,type=int,help="number of filter in convolution layer 2")
parser.add_argument("--gamma", default=2.4, type=float, help="Gamma value")
parser.add_argument("--learning_rate", default=0.01, type=float, help="Learning rate")
parser.add_argument("--batch_size", default=64, type=int, help="Batch Size")
parser.add_argument("--neg_ratio", default=1, type=float, help="Number of negative triples generated by positive")
parser.add_argument("--num_epochs", default=3000, type=int, help="Number of training epochs")
parser.add_argument("--num_splits", default=8, type=int, help="Split the validation set into 8 parts for a faster evaluation")
parser.add_argument("--testIdx", default=1, type=int, help="From 0 to 7. Index of one of 8 parts")
parser.add_argument('--fold',default='1')
parser.add_argument("--dpo1", action='store_true',help="use dropout in CNN1")
parser.add_argument("--dpo2", action='store_true',help="use dropout in CNN2")
parser.add_argument("--bn1", action='store_true',help="use batch normalization in CNN1")
parser.add_argument("--bn2", action='store_true',help="use batch normalization in CNN1")
parser.add_argument("--alpha",default = 0.0)

#parser.add_argument("--run_folder", default="../", help="Data sources.")
#parser.add_argument("--filter_sizes", default="1", help="Comma-separated filter sizes")
#parser.add_argument("--num_filters", default=500, type=int, help="Number of filters per filter size")
#parser.add_argument("--dropout_keep_prob", default=1.0, type=float, help="Dropout keep probability")
#parser.add_argument("--l2_reg_lambda", default=0.001, type=float, help="L2 regularization lambda")
#parser.add_argument("--is_trainable", default=True, type=bool, help="")
#parser.add_argument("--saveStep", default=200, type=int, help="")
#parser.add_argument("--allow_soft_placement", default=True, type=bool, help="Allow device soft device placement")
#parser.add_argument("--log_device_placement", default=False, type=bool, help="Log placement of ops on devices")

parser.add_argument("--useConstantInit", action='store_true')

#parser.add_argument("--model_index", default='200', help="")
parser.add_argument("--decode", action='store_false')

args = parser.parse_args()
# print(args)


# Load data
#print("Loading data...")

# train, valid, test, words_indexes, indexes_words, \
# headTailSelector, entity2id, id2entity, relation2id, id2relation = build_data(path=args.data, name=args.name)

fold = args.fold
train, test, words_indexes, indexes_words, headTailSelector, entity2id, id2entity, relation2id, id2relation \
    = custom_build_data(name=args.name,path='./data',fold=fold)

# exit(0)
data_size = len(train)
train_batch = Batch_Loader(train, words_indexes, indexes_words, headTailSelector, \
                           entity2id, id2entity, relation2id, id2relation, batch_size=args.batch_size,
                           neg_ratio=args.neg_ratio)

entity_array = np.array(list(train_batch.indexes_ents.keys()))
#print("Loading data... finished!")

x_train = np.array(list(train.keys())).astype(np.int32)
y_train = np.array(list(train.values())).astype(np.float32)
len_train = len(x_train)
batch_train = int(len_train / (args.num_splits - 1))

# x_valid = np.array(list(valid.keys())).astype(np.int32)
# y_valid = np.array(list(valid.values())).astype(np.float32)
# len_valid = len(x_valid)
# batch_valid = int(len_valid / (args.num_splits - 1))

# x_test = np.array(list(test.keys())).astype(np.int32)
# y_test = np.array(list(test.values())).astype(np.float32)
# len_test = len(x_test)
# batch_test = int(len_test / (args.num_splits - 1))
#batch_test = args.batch_size


# print(id2entity)
# entityid = list(id2entity.keys())
# exit(0)
# uncomment when tuning hyper-parameters on the validation set

# x_test = x_valid
# y_test = y_valid
# len_test = len_valid
# batch_test = batch_valid

# print(len_test)
# exit(0)
##########################################

# path of text file that store evaluation result
file_path = os.path.join(os.getcwd(),'experiment_result',args.model_name,args.name+fold)

# for WN18RR dataset
# rel_dict = {
#             1:'_hypernym',
#             4:'_derivationally_related_form',
#             9:'_instance_hypernym',
#             26:'_also_see',
#             35:'_member_meronym',
#             78:'_synset_domain_topic_of',
#             89:'_has_part',
#             112:'_member_of_domain_usage',
#             141:'_member_of_domain_region',
#             549:'_verb_group',
#             862:'_similar_to'
#         }

# for Algo-Citation Dataset
# rel_dict = {
#     1: 'EXTEND',
#     4: 'USE',
#     13: 'MENTION'
# }


if args.decode == False:
    #_file = checkpoint_prefix + "-" + _model_index
    #get file path here
    lstHT = []
    for _index in range(args.num_splits):
        with open(file_path + '/eval.test.' + str(_index) + '.txt') as f:
            for _line in f:
                if _line.strip() != '':
                    lstHT.append(list(map(float, _line.strip().split())))
    lstHT = np.array(lstHT)
    print('mr, mrr, hits@10 --> ', np.sum(lstHT, axis=0) / (2 * len_test))
    #print(file_path, 'mr, mrr, hits@10 --> ', np.sum(lstHT, axis=0) / (2 * len_test))
    #print('------------------------------------')
    exit(0) # exit program

# print(len(words_indexes))
# print(len(entity2id))
# exit(0)

# load model here
kge_model = KGEModel(
    #model_name=args.model,
    #hidden_dim=args.hidden_dim,
    #hidden_dim=50,  # just for debugging (remove this line later)
    vocab_size=len(words_indexes),
    embedding_size=args.embedding_dim,
    gamma=args.gamma,
    #gamma=2.4, # just for debugging (remove this line later)
    batch_size = args.batch_size,
    neg_ratio=args.neg_ratio,
    dpo1=args.dpo1,
    dpo2=args.dpo2,
    bn1 = args.bn1,
    bn2 = args.bn2,
    channel1_num=args.ConvL1FiltersNum,
    channel2_num=args.ConvL2FiltersNum
)

#  args.name, args.model_name+fold+'-'+str(epoch+1)+'epochs'
model_path = os.path.join(os.getcwd(),'models',args.name+'_'+args.model_name+fold+'-'+str(args.num_epochs)+'epochs'+'.pth')
# model_path = os.path.join(os.getcwd(),'models',args.name+'_'+args.model_name+fold+'.pth')
kge_model.load_state_dict(torch.load(model_path))
kge_model = kge_model.cuda()

# set all parameters not to require grad
for param in kge_model.parameters():
    param.requires_grad = False

kge_model.eval()
#print(kge_model)
print('load model finished')
# print(x_test)
# print(y_test)
# exit(0)
#print(kge_model)

'''
    store cited papers to citing paper objects
'''

all_PaperRank_score = {}

# CiteULike_5_fold_fold1
with open('PaperRank score/' + args.name + "_fold" + args.fold + '.pkl', 'rb') as f:
    all_PaperRank_score = pickle.load(f)

# print(all_PaperRank_score)
# exit(0)

test_papers = {}

with open(args.data+'/'+args.name+'/'+'test_fold'+args.fold+'.txt') as f:
    lines = f.readlines()

# print(lines)
# exit(0)

# test_data = 555
for line in lines:
    xt = line.replace('\n','').split(sep='\t')
    citing_id = int(xt[0])
    cited_id = int(xt[2])
    if citing_id not in test_papers:
        test_papers[citing_id] = Paper(citing_id)
    test_papers[citing_id].add_test_cited_paper(int(cited_id))

with open(args.data+'/'+args.name+'/'+'train_fold'+args.fold+'.txt') as f:
    lines = f.readlines()

for line in lines:
    xt = line.replace('\n','').split(sep='\t')
    citing_id = int(xt[0])
    cited_id = int(xt[2])
    if citing_id in test_papers:
        test_papers[citing_id].add_train_cited_paper(int(cited_id))


def predict_score(x_batch):
    h = torch.tensor(x_batch[:,0],dtype=torch.long).cuda().reshape(-1,1)
    r = torch.tensor(x_batch[:, 1],dtype=torch.long).cuda().reshape(-1,1)
    t = torch.tensor(x_batch[:, 2],dtype=torch.long).cuda().reshape(-1,1)

    return kge_model(h,r,t,True).cpu().detach().numpy()

# score list is list of implausibility score of each triplet

def recommend_paper(cited_papers,score_dict,k=10):
    bpref = 0.0
    mrr = 0.0
    min_rank = 99999

    PrecAtK = {}
    RecAtK = {}
    F1AtK = {}

    ranked_paper = []

    for i in range(1, k+1):
        PrecAtK[i] = 0
        RecAtK[i] = 0
        F1AtK[i] = 0

    # for i in range(0,len(score_list)):
    #     score_dict[i] = score_list[i][0]

    # sorted_score_dict = sorted(score_dict.items(), key=itemgetter(1), reverse=False)
    sorted_score_dict = sorted(score_dict.items(), key=itemgetter(1), reverse=True)
    score_rank = rankdata(list(score_dict.values())) # original code...
    score_rank = len(score_rank) - score_rank +1
    # score_rank = rankdata([-1 * i for i in list(score_dict.values())]).astype(int) # higher score has higher rank but rankdata not have inverse rank
    score_rank_dict = {}

    score_dict_key = list(score_dict.keys())

    for i in range(len(score_dict)):
        score_rank_dict[score_dict_key[i]] = int(score_rank[i])

    for pc in cited_papers:
        if score_rank_dict[pc] < min_rank:
            min_rank = score_rank_dict[pc]
    #
    if min_rank > k:  # K = 10 (may change later)
        mrr = 0
    else:
        mrr = 1 / min_rank

    for a in range(0,len(cited_papers)):
        r_higher = 0
        # for b in range(0,len(sorted_score_dict)[:10]):  # K = 10 (may change later)
        for b in range(0, k):
            if cited_papers[a] == int(sorted_score_dict[b][0]):
            # if cited_papers[a] == int(sorted_score_dict.get(b)[0]):
                break
            if cited_papers[a] != int(sorted_score_dict[b][0]) and int(sorted_score_dict[b][0]) not in cited_papers:
            # if cited_papers[a] != int(sorted_score_dict.get(b)[0]) and int(sorted_score_dict.get(b)[0]) not in cited_papers:
                r_higher = r_higher + 1
        # print(cited_papers[a], '-> score rank: ',score_rank[cited_papers[a]],'-> r higher:', r_higher)
        # bpref = bpref + (1 - r_higher / len(score_dict))
        bpref = bpref + (1 - r_higher / k)
    bpref = bpref / len(cited_papers)


    for i in range(1, k+1):
        pr = len(set(cited_papers) & set(list(dict(sorted_score_dict).keys())[:i])) / i
        rec = len(set(cited_papers) & set(list(dict(sorted_score_dict).keys())[:i])) / len(cited_papers)
        F1 = (2 * pr * rec) / (pr + rec) if pr + rec > 0 else 0
        PrecAtK[i] = PrecAtK[i] + pr
        RecAtK[i] = RecAtK[i] + rec
        F1AtK[i] = F1AtK[i] + F1

    return PrecAtK, RecAtK, F1AtK, mrr, bpref

pr_allpaper = {}
rec_allpaper = {}
f1_allpaper = {}
mrr_allpapaer = 0
bpref_allpaper = 0

k = 20 # numbers to find precision@k, recall@k, ...@k

for i in range(1,k+1):
    pr_allpaper[i] = 0
    rec_allpaper[i] = 0
    f1_allpaper[i] = 0

print('start evaluation')
start = time.time()
totaltime = 0.0
count_paper = 1

result = ''
alpha = float(args.alpha) # in range [0,1] for ensemble score


for p in test_papers.keys():
    citing_id = test_papers[p].id
    cited_ids = test_papers[p].test_cited_paper
    # triplets = np.ones((len(id2entity),3))
    triplets = np.tile(citing_id,(len(id2entity),3))
    triplets[:,1] = 1
    triplets[:,2] = list(id2entity.keys())
    del_idx = test_papers[p].train_cited_paper
    # del_idx.append(citing_id)
    triplets = np.delete(triplets,(del_idx),axis=0)
    triplets = triplets.astype(int)
    score = predict_score(triplets)
    score = 1-((score - np.min(score)) / np.ptp(score)) # 1 - normalized score

    score_dict = {} # score of CNN
    # CF_score_dict = {} # score of CF
    combined_score = {} # ensemble score...
    for i in range(len(triplets)):
        score_dict[triplets[i,2]] = score[i][0]

    # for idx in del_idx:
    #     del score_dict[idx]


    # get score from PaperRank here...
    PaperRank_score = all_PaperRank_score[p]
    for cited_paper in score_dict.keys():
        if str(cited_paper) not in PaperRank_score:
            PaperRank_score[str(cited_paper)] = 0.0
    # print('len after adding cited paper:', len(PaperRank_score))
    PaperRank_score_val = list(np.float_(list(PaperRank_score.values())))    # list(np.float_(list_name))

    PaperRank_score_key = list(PaperRank_score.keys())
    PaperRank_score_normalized_val = (PaperRank_score_val - np.min(PaperRank_score_val)) / np.ptp(PaperRank_score_val) if np.ptp(PaperRank_score_val) != 0 else [0]*len(PaperRank_score_val)# score of PaperRank is normalized here...

    for i in range(0,len(PaperRank_score)):
        PaperRank_score[PaperRank_score_key[i]] = PaperRank_score_normalized_val[i]

    # print(PaperRank_score)
    # print('final len:',len(PaperRank_score_val))
    # exit(0)

    # calculate ensemble score here (alpha*ConvCN + (1-alpha)*CF)
    # loop through score_dict
    for cited_paper in score_dict.keys():
        # print('ConvCN score:',score_dict[cited_paper], 'CF score', CF_score.get(str(cited_paper),0))

        # normalize this score !!!!!!!!!! (but how)
        # may use min-max normalization
        # then 1 - normalized score
        # ConvCN_scr = -1*score_dict[cited_paper] # negative value has higher rank
        ConvCN_scr = score_dict[cited_paper]
        PaperRank_scr = PaperRank_score.get(str(cited_paper), 0)

        combined_score[cited_paper] = alpha*PaperRank_scr + (1-alpha)*ConvCN_scr # weighted average

    PrecAtK, RecAtK, F1AtK, mrr, bpref = recommend_paper(cited_ids,combined_score,k) # store it in global var???

    for i in range(1, k+1):
        pr_allpaper[i] = pr_allpaper[i] + PrecAtK[i]
        rec_allpaper[i] = rec_allpaper[i] + RecAtK[i]
        f1_allpaper[i] = f1_allpaper[i] + F1AtK[i]

    mrr_allpapaer = mrr_allpapaer+mrr
    bpref_allpaper = bpref_allpaper+bpref

    # recommended_papers, implausibility_scores = recommend_paper(cited_ids, score_dict)
    # result = result+str(citing_id)+'\t'+ ','.join(map(str, recommended_papers)) + '\t' + ','.join(map(str, implausibility_scores)) +'\n'
    # print(result)
    print('Dataset:',args.name,' fold:',args.fold, ' citing paper id:',citing_id, '->', count_paper,'/',len(test_papers))
    count_paper = count_paper+1

    # if count_paper == 5:
    #     exit(0)

for i in range(1, k+1):
    pr_allpaper[i] = pr_allpaper[i]/len(test_papers)
    rec_allpaper[i] = rec_allpaper[i]/len(test_papers)
    f1_allpaper[i] = f1_allpaper[i]/len(test_papers)

mrr_allpapaer = mrr_allpapaer/len(test_papers)
bpref_allpaper = bpref_allpaper/len(test_papers)

end = time.time()
totaltime = end-start

# write result to file...
'''
file format:
    Prc@k
    Rec@k
    F1@k
    MRR, Bpref
'''

result = ''

for pr in pr_allpaper.values():
    # print(pr, end=' ')
    result = result + str(pr) + '\t'
result = result + '\n'
# print()
for rec in rec_allpaper.values():
    # print(rec, end=' ')
    result = result + str(rec) + '\t'
result = result + '\n'
for f1 in f1_allpaper.values():
    # print(f1, end=' ')
    result = result + str(f1) + '\t'
result = result + '\n'
# # print()
# # print(mrr_allpapaer,'/t',bpref_allpaper)
result = result + str(mrr_allpapaer) + '\t' + str(bpref_allpaper) + '\n'
print(result)

#  args.name, args.model_name+fold+'-'+str(epoch+1)+'epochs'
# model_path = os.path.join(os.getcwd(),'models',args.name+'_'+args.model_name+fold+'-'+str(args.num_epochs)+'epochs'+'.pth')
# path = '\\result\\'+args.model_name+'\\alpha='+alpha
# os.makedirs(path)

path = os.path.join(os.getcwd(),'result','ConvCN_PaperRank_weighted_avg',args.model_name,'alpha='+str(alpha))

if not os.path.exists(path):
    os.makedirs(path)

result_file = open(path+'\\ensemble result_'+args.name+'_'+args.model_name+fold+'-'+str(args.num_epochs)+'epochs.txt','w')
result_file.write(result)
# print(totaltime)
exit(0)

